(this["webpackJsonppraveenoruganti-courses"]=this["webpackJsonppraveenoruganti-courses"]||[]).push([[236],{805:function(e,n,a){"use strict";a.r(n),n.default=' Apache Kafka is a distributed streaming platform with capabilities such as publishing and subscribing to a stream of records, storing the records in a fault tolerant way, and processing that stream of records.\n\nIt is used to build real-time streaming data pipelines, that can perform functionalities such as reliably passing a stream of records from one application to another and processing and transferring the records to the target applications.\n\n\n**Kafka Architecture**\n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/kafkaarchitecture1.png)\n\n\n**Kafka Features**\n\n- **High throughput**\xa0: Provides support for hundreds of thousands of messages with modest hardware.\n- **Scalability**\xa0: Highly scalable distributed systems with no downtime.\n- **Data loss**\xa0: Kafka ensures no data loss once configured properly.\n- **Stream Processing**\xa0: Kafka can be used along with real time streaming applications like Spark and Storm.\n- **Durability**\xa0: Provides support for persisting messages to disk.\n- **Replication**\xa0: Messages can be replicated across clusters, which supports multiple subscribers.\n\n**Kafka components**\n\n**Topic**\n\nA topic is a category or feed or named stream to which records are published. Kafka stores topic in logs file.\n\n- It is simple to table in database (without all constraints).\n- You can have as many as topics you want.\n- A topic is identified by its name.\n\n**Partitions**\n\nTopics are broken up into ordered commit logs called partitions. Kafka spreads those log\'s partitions across multiple servers or disks.\n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/kafka.png)\n\nThe partition maintains the order in which data was inserted and once the record is published to the topic, it remains there depending on the retention period (which is configurable). The records are always appended at the end of the partitions. It maintains a flag called \'offsets,\' which uniquely identifies each record within the partition.\n\nThe offset is controlled by the consuming applications. Using offset, consumers might backtrace to older offsets and reprocess the records if needed.\n\n- Topics are split into partitions.\n  a) Each partition is ordered.\n  b) Each message in partition gets an incremental id called offset. \n\n**Offset**\n\n- Offset only have a meaning for a specific partition. for example, offset 3 in partition 0 doesn\'t represent the same data as offset 3  in partition 1.  \n- Order is guaranteed only with a partition (not across partitions).\n- Data is kept only for a limited time(default is one week).\n- Once the data is written to a  partition, it can\'t be changed (immutability).\n\n`For topic example, truck_gps`\n\n- Say you have a fleet of trucks, each truck reports its GPS position to kafka.\n- You can topic `truck_gps` the contains position of all trucks.\n- Each truck will send a message to Kafka every 20 seconds, each message will contain the truck ID and the truck position (latitude and longitude).\n- We choose to create that topic with 10 partitions (arbitrary number).\n- Data is assigned randomly to a partition unless a key is provided.\n\n**Broker**\n\n- Kafka cluster typically consists of multiple brokers (servers) to maintain load balance. \n- Each broker is identified with its ID (integer).\n- Each broker contains certain topic partitions.\n- After connecting to any broker (called a bootstrap broker), you will be connected to the entire cluster.\n- A good number to get started is 3 brokers, but some big clusters have over 100 brokers. \n- Kafka brokers are stateless, so they use ZooKeeper for maintaining their cluster state. \n- One Kafka broker instance can handle hundreds of thousands of reads and writes per second and each broker can handle TB of messages without performance impact. \n- Kafka broker leader election can be done by ZooKeeper.\n\n**Topic replication factor**\n\n- Topics should have a replication factor > 1 (usually between 2 and 3).\n- This way if the broker is down, another broker can serve the data.\n\n**Producers**\n\n- Producers write data to topics (which is made of partition).\n- Producers automatically know to which broker and partition to write to.\n- In case of Broker failures, Producers will automatically recover.\n- Producers can choose to receive acknowledgement of data writes:\n  \n  a) `acks=0`, Producer wont wait for acknowledgement(possible data loss).\n\n  b) `acks=1`, Producer will wait for leader acknowledgement(minimal data loss).\n\n  c) `acks=all`, Leader + replicas acknowledgement(no data loss).\n\n`Producers: Message keys`\n\n- Producers can choose to send a key with the message (string, number etc).\n- If key=null, data is send round robin (broker 101 then 102 then 103 etc).\n- If key is sent, then all the messages for that key will always go to the same partition.\n- A key is basically sent if you need message ordering for a specific field (ex: truck_id).\n\n**Consumers**\n\n- Consumers read data from a topic (identified by a name).\n- Consumers know which broker to read from.\n- In case of broker failures, consumers know how to recover.\n- Data is read in order within each partitions.\n\n`Consumer Groups`\n\n- Consumer read data in consumer groups.\n- Each Consumer with in a group reads from exclusive paritions.\n- If you have more consumers then partitions, some consumers will be inactive.\n\n`Consumer Offsets`\n\n- Kafka stores the offsets at which a consumer group has been reading.\n- The offsets committed live in a Kafka topic named _consumer_offsets.\n- When a consumer in a group has processed data received from Kafkam it should be committing the offsets.\n- If a consumer dies, it will be able to read back from where it let off thanks to the committed consumer offsets.\n  \n`Delivery semantics for Consumers`\n\n- Consumers choose when to commit offsets.\n- There are 3 delivery semantics\n  \n  a) `At most once`\n\n     - offsets are committed as soon as message is received.\n     - If the processing goes wrong, the message will be lost(it won\'t be read again).\n  \n  b) `At least once (usually preferred)`\n\n     - offsets are committed after the message is processed.\n     - If the process goes wrong, the message will be read again.\n     - This can result in duplicate processing of messages. Make sure your processing is idempotent (i.e.. processing again the messages won\'t impact your systems).\n  \n  c) `Exactly once`\n\n     - Can be achieved for Kafka => Kafka workflows using Kafka Streams API. \n     - For Kafka => External System workflows, use an idempotent consumer.\n\n**Kafka Broker Discovery**\n\n- Every Kafka broker is also called as `bootstrap server`.\n- That means that `you only need to connect to one broker`, and you will be connected to the entire cluster.\n- Each broker knows about all brokers, topics, partitions (metadata).\n\n**Zookeeper**\n\n- Zookeeper manages brokers (keep a list of them).\n- Zookeeper helps in performing leader election for partitions.\n- Zookeeper sends notifications to Kafka in case of changes (ex: new topic, broker dies, broker comes up, delete topics, etc...).\n- `Kafka can\'t work without Zookeeper`\n- Zookeeper by design operates with an odd number of servers (3, 5, 7).\n- Zookeeper has a leader (handle writes), the rest of the servers are followers (handle reads).\n- Zookeeper does NOT store consumer offsets with Kafka > v0.10.\n\n**Kafka Guarantees**\n\n- Messages are appended to  a topic-partition in the order they are sent.\n- Consumers read messages in the order sorted in a topic-partition.\n- With a replication factor of N, producers and consumers can tolerate upto N-1 brokers being down.\n- This is why a replication factor of 3 is a good idea:\n  - Allows for one broker to be taken down for maintenance.\n  - Allows for another broker to be taken down unexpectedly.\n- As long as the number of partitions remains constant for a topic (no new partitions), the same key will always go to the same partition.\n\n\n**What problem does Kafka resolve?**\n\nWithout any messaging queue implementation, what the communication between client nodes and server nodes look alike is shown below:\n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/1.jpg)\n\nThere is a large numbers of data pipelines which are used for communication. It is very difficult to update this system or add another node.\n\nIf we use Kafka, then the entire system will look like something\n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/2.jpg)\n\nSo, all the client servers will send messages to topics in Kafka and all backend servers will consume messages from kafka topics.\n\n**Spring Support for Kafka**\n\nSpring provides good support for Kafka and provides the abstraction layers to work with over the native Kafka Java clients.\n\nWe can add the below dependencies to get started with Spring Boot and Kafka\n\n```jsx\n<dependency>\n  <groupId>org.springframework.kafka</groupId>\n  <artifactId>spring-kafka</artifactId>\n</dependency>\n```\n\n**Steps to download and run Apache Kafka in local**\n\n- Download the apache kafka and unzip using winrar\n\n[https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)\n\n- Once you download Kafka, you can issue a command to start ZooKeeper which is used by Kafka to store metadata.\n\n```jsx\n\nD:/praveenoruganti/softwares/kafka>bin/windows/zookeeper-server-start.bat config/zookeeper.properties\n\n```\n\n- Next, we need to start the Kafka cluster locally by issuing the below command.\n\n```jsx\n\nD:/praveenoruganti/softwares/kafka>bin/windows/kafka-server-start.bat ./config/server.properties\n\n```\n\nNow, by default, the Kafka server starts on localhost:9092.\n\nLet\u2019s develop a simple REST controller with Swagger integration and expose with one endpoint, /publish, as shown below. It is used to publish the message to the topic. \n\n```jsx\n@RestController\n@ApiOperation(tags = "Apache Kafka Restful Service", value = "KafkaController")\npublic class KafkaController {\n\n\t@Autowired\n\tProducerService producerService;\n\n\t@PostMapping(value = "/publish")\n\t@ApiOperation(value = "Publishing Message")\n\tpublic void sendMessageToKafkaTopic(@RequestParam("message") String message) {\n\t\tthis.producerService.sendMessage(message);\n\t}\n\n}\n```\n\nWe can then write the producer which uses Spring\'s KafkaTemplate to send the message to a topic named users, as shown below.\n\n```jsx\n@Service\n@Slf4j\npublic class ProducerService {\n\t@Autowired\n\tprivate KafkaTemplate<String, String> kafkaTemplate;\n\n\tpublic void sendMessage(String message) {\n\t\tthis.kafkaTemplate.send("users", message);\n\t\tlog.info("Message Sent", v("MESSAGE", message));\n\t}\n}\n\n```\n\nWe can also write the consumer as shown below, which consumes the message from the topic users and output the logs to the console.\n\n```jsx\n@Service\n@Slf4j\npublic class ConsumerService {\n\n\t@KafkaListener(topics = "users", groupId = "group_id")\n\tpublic void consume(String message) {\n\t\tlog.info("Message Received", v("MESSAGE", message));\n\t}\n}\n\n```\n\nNow, we need a way to tell our application where to find the Kafka servers and create a topic and publish to it. We can do it using application.yml as shown below. \n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/kafka1.png)\n\nNow, if we run the application and hit the endpoint as shown below, we have published a message to the topic.\n\n![screenshot of the app](https://praveenoruganti.github.io/courses/images/springboot/kafka2.png)\n\nNow, if we check the logs from the console, it should print the message which was sent to the publish endpoint. '}}]);